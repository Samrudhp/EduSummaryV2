# üîÑ Model Caching in EduSummary

## Overview

EduSummary uses **automatic model caching** to ensure models are downloaded **once** and reused on every subsequent run. This dramatically improves startup time and reduces bandwidth usage.

---

## üéØ How It Works

### First Run
1. Models are downloaded from the internet
2. Models are cached in `~/.cache/` directory
3. Future runs use these cached models

### Subsequent Runs
1. System checks `~/.cache/` for existing models
2. If found, loads from cache (instant)
3. If not found, downloads and caches

**Result:** Models download only once, no matter how many times you restart the backend!

---

## üìÅ Cache Locations

### 1. Embeddings Model (Sentence Transformers)
- **Model**: `all-mpnet-base-v2`
- **Size**: ~300MB
- **Cache Location**: `~/.cache/huggingface/hub/`
- **Full Path**: `~/.cache/huggingface/hub/models--sentence-transformers--all-mpnet-base-v2/`

### 2. Language Model (GPT4All)
- **Model**: `orca-mini-3b-gguf2-q4_0.gguf`
- **Size**: ~2GB
- **Cache Location**: `~/.cache/gpt4all/`
- **Full Path**: `~/.cache/gpt4all/orca-mini-3b-gguf2-q4_0.gguf`

---

## üîç Verify Cached Models

### Check if models are cached:

```bash
# Check embeddings model
ls -lh ~/.cache/huggingface/hub/

# Check GPT4All model
ls -lh ~/.cache/gpt4all/
```

### Check cache size:

```bash
# Total HuggingFace cache
du -sh ~/.cache/huggingface/

# Total GPT4All cache
du -sh ~/.cache/gpt4all/

# Total cache for both
du -sh ~/.cache/huggingface/ ~/.cache/gpt4all/
```

---

## üöÄ Pre-download Models (Optional)

You can pre-download models before running the backend:

```bash
cd backend
source venv/bin/activate
python setup_models.py
```

This script:
- ‚úÖ Downloads embeddings model to `~/.cache/huggingface/`
- ‚úÖ Downloads GPT4All model to `~/.cache/gpt4all/`
- ‚úÖ Verifies both are ready to use
- ‚úÖ Shows cache locations

---

## ‚è±Ô∏è Performance Comparison

### Without Caching (re-downloading every time)
- First run: 10-15 minutes (download + load)
- Second run: 10-15 minutes (download + load)
- Third run: 10-15 minutes (download + load)
- **Total for 3 runs**: 30-45 minutes + 6.9GB bandwidth

### With Caching (current implementation)
- First run: 10-15 minutes (download + cache + load)
- Second run: 30-60 seconds (load from cache)
- Third run: 30-60 seconds (load from cache)
- **Total for 3 runs**: 12-17 minutes + 2.3GB bandwidth

**Savings**: ~70% time saved, ~67% bandwidth saved!

---

## üîß Implementation Details

### Embeddings Model Caching

```python
# In backend/services/rag_service.py
def _initialize_embeddings(self):
    # Check if already loaded in memory
    if self.embeddings is not None:
        print("Using cached instance")
        return
    
    # HuggingFace automatically uses ~/.cache/huggingface/
    cache_folder = os.path.expanduser("~/.cache/huggingface/hub")
    self.embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-mpnet-base-v2",
        cache_folder=cache_folder  # Explicit cache location
    )
```

### GPT4All Model Caching

```python
# In backend/services/rag_service.py
def _initialize_llm(self):
    # Check if already loaded in memory
    if self.llm is not None:
        print("Using cached instance")
        return
    
    # GPT4All automatically uses ~/.cache/gpt4all/
    model_name = "orca-mini-3b-gguf2-q4_0.gguf"
    
    # Check cache before loading
    gpt4all_cache = os.path.expanduser("~/.cache/gpt4all/")
    if os.path.exists(os.path.join(gpt4all_cache, model_name)):
        print(f"Using cached model from {gpt4all_cache}")
    
    # Load from cache or download if needed
    self.llm = GPT4All(model=model_name)
```

---

## üßπ Managing Cache

### View Cache Information

```bash
# List all cached models
ls -la ~/.cache/huggingface/hub/
ls -la ~/.cache/gpt4all/

# Check disk usage
df -h ~
```

### Clear Cache (if needed)

**Warning:** This will delete cached models. They will be re-downloaded on next run.

```bash
# Clear embeddings cache only
rm -rf ~/.cache/huggingface/

# Clear GPT4All cache only
rm -rf ~/.cache/gpt4all/

# Clear both caches
rm -rf ~/.cache/huggingface/ ~/.cache/gpt4all/

# Re-download models
cd backend
python setup_models.py
```

### Move Cache to Different Location

If you want to store models elsewhere (e.g., external drive):

```bash
# Create symlink to new location
mv ~/.cache/huggingface /path/to/external/drive/huggingface
ln -s /path/to/external/drive/huggingface ~/.cache/huggingface

mv ~/.cache/gpt4all /path/to/external/drive/gpt4all
ln -s /path/to/external/drive/gpt4all ~/.cache/gpt4all
```

---

## üêõ Troubleshooting

### Problem: Models seem to re-download every time

**Cause**: Cache directory permissions or missing cache

**Solution**:
```bash
# Check if cache directories exist
ls ~/.cache/huggingface/
ls ~/.cache/gpt4all/

# Check permissions
ls -la ~/.cache/

# Fix permissions if needed
chmod -R 755 ~/.cache/huggingface/
chmod -R 755 ~/.cache/gpt4all/

# Re-run setup
cd backend
python setup_models.py
```

### Problem: "Permission denied" when accessing cache

**Solution**:
```bash
# Fix ownership
sudo chown -R $USER:$USER ~/.cache/huggingface/
sudo chown -R $USER:$USER ~/.cache/gpt4all/
```

### Problem: Disk space full

**Solution**:
```bash
# Check space
df -h ~

# Clear old models (they'll re-download)
rm -rf ~/.cache/huggingface/
rm -rf ~/.cache/gpt4all/

# Or move to larger drive (see above)
```

### Problem: Corrupted cache

**Symptoms**: Load errors, incomplete files

**Solution**:
```bash
# Delete corrupted cache
rm -rf ~/.cache/huggingface/
rm -rf ~/.cache/gpt4all/

# Re-download clean copies
cd backend
python setup_models.py
```

---

## üìä Cache Statistics

### After First Setup

```bash
$ du -sh ~/.cache/huggingface/
300M    ~/.cache/huggingface/

$ du -sh ~/.cache/gpt4all/
2.0G    ~/.cache/gpt4all/

$ du -sh ~/.cache/huggingface/ ~/.cache/gpt4all/
300M    ~/.cache/huggingface/
2.0G    ~/.cache/gpt4all/
Total: 2.3G
```

---

## üéì Best Practices

1. **First Setup**
   - Run `python setup_models.py` to pre-download
   - Verify cache with `ls ~/.cache/`
   - Check total size with `du -sh ~/.cache/`

2. **Regular Use**
   - Models load from cache automatically
   - No manual intervention needed
   - Cache persists across reboots

3. **Updates**
   - If you update model versions, clear old cache
   - New models will cache automatically

4. **Backups**
   - Include `~/.cache/huggingface/` and `~/.cache/gpt4all/` in backups
   - Restore cache to avoid re-downloading

5. **Sharing**
   - Copy cache folders to other machines
   - Place in `~/.cache/` on target machine
   - Skip download step entirely!

---

## üîê Security Note

Cache directories (`~/.cache/`) are:
- User-specific (not shared between users)
- Protected by file system permissions
- Not accessible to other users
- Safe to store models

---

## ‚úÖ Summary

| Feature | Status |
|---------|--------|
| Automatic caching | ‚úÖ Enabled |
| Re-download prevention | ‚úÖ Working |
| Cache persistence | ‚úÖ Across reboots |
| Memory efficiency | ‚úÖ Single instance |
| Disk efficiency | ‚úÖ Shared cache |
| Easy cleanup | ‚úÖ Simple rm command |

---

**Your models are now cached and will never need to be re-downloaded!** üéâ

For more information, see:
- `backend/README.md` - Backend setup
- `backend/setup_models.py` - Pre-download script
- `SETUP.md` - Complete setup guide
